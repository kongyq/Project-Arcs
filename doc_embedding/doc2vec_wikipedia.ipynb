{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec to wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conduct the replication to **Document Embedding with Paragraph Vectors** (http://arxiv.org/abs/1507.07998).\n",
    "In this paper, they showed only DBOW results to Wikipedia data. So we replicate this experiments using not only DBOW but also DM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import Doc2Vec module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dump of all Wikipedia articles from [here](http://download.wikimedia.org/enwiki/) (you want the file enwiki-latest-pages-articles.xml.bz2, or enwiki-YYYYMMDD-pages-articles.xml.bz2 for date-specific dumps).\n",
    "\n",
    "Second, convert the articles to WikiCorpus. WikiCorpus construct a corpus from a Wikipedia (or other MediaWiki-based) database dump.\n",
    "\n",
    "For more details on WikiCorpus, you should access [Corpus from a Wikipedia dump](https://radimrehurek.com/gensim/corpora/wikicorpus.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import MmCorpus\n",
    "wikidump = \"F:/Corpus/wikidump/enwiki-latest-pages-articles.xml.bz2\"\n",
    "#wikidump = \"F:/Corpus/wikidump/enwiki-20110115-pages-articles.xml.bz2\"\n",
    "wiki = WikiCorpus(wikidump, dictionary={})\n",
    "wikicorpus = \"F:/Corpus/wikidump/wiki_corpus.mm\"\n",
    "#MmCorpus.serialize(wikicorpus, wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "wiki=MmCorpus(wikicorpus)\n",
    "\n",
    "#MmCorpus.load(wikicorpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define **TaggedWikiDocument** class to convert WikiCorpus into suitable form for Doc2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class TaggedWikiDocument(object):\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "        self.wiki.metadata = True\n",
    "    def __iter__(self):\n",
    "        for content, (page_id, title) in self.wiki.get_texts():\n",
    "            yield TaggedDocument(content, [title])\n",
    "            # yield TaggedDocument([c.decode(\"utf-8\") for c in content], [title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "documents = TaggedWikiDocument(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing\n",
    "To set the same vocabulary size with original paper. We first calculate the optimal **min_count** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\project arcs\\lib\\site-packages\\gensim\\utils.py:1254: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  Split `corpus` into chunks of this size.\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d84b944d74db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDoc2VecVocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc2VecVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# pre.scan_vocab(corpus_file=)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\project arcs\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, documents, corpus_file, docvecs, progress_per, trim_rule)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\project arcs\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[1;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'count'"
     ],
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'count'",
     "output_type": "error"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2VecVocab\n",
    "pre = Doc2VecVocab(min_count=0)\n",
    "pre.scan_vocab(documents=documents)\n",
    "# pre.scan_vocab(corpus_file=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 0, size of vocab:  8545782.0\n",
      "min_count: 1, size of vocab:  8545782.0\n",
      "min_count: 2, size of vocab:  4227783.0\n",
      "min_count: 3, size of vocab:  3008772.0\n",
      "min_count: 4, size of vocab:  2439367.0\n",
      "min_count: 5, size of vocab:  2090709.0\n",
      "min_count: 6, size of vocab:  1856609.0\n",
      "min_count: 7, size of vocab:  1681670.0\n",
      "min_count: 8, size of vocab:  1546914.0\n",
      "min_count: 9, size of vocab:  1437367.0\n",
      "min_count: 10, size of vocab:  1346177.0\n",
      "min_count: 11, size of vocab:  1267916.0\n",
      "min_count: 12, size of vocab:  1201186.0\n",
      "min_count: 13, size of vocab:  1142377.0\n",
      "min_count: 14, size of vocab:  1090673.0\n",
      "min_count: 15, size of vocab:  1043973.0\n",
      "min_count: 16, size of vocab:  1002395.0\n",
      "min_count: 17, size of vocab:  964684.0\n",
      "min_count: 18, size of vocab:  930382.0\n",
      "min_count: 19, size of vocab:  898725.0\n"
     ]
    }
   ],
   "source": [
    "for num in range(0, 20):\n",
    "    print('min_count: {}, size of vocab: '.format(num), pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, they set the vocabulary size 915,715. It seems similar size of vocabulary if we set min_count = 19. (size of vocab = 898,725)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the Doc2Vec Model\n",
    "To train Doc2Vec model by several method, DBOW and DM, we define the list of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, vector_size=300, window=8, min_count=50, epochs=10, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    # Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=8, min_count=10, epochs=10, workers=cores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\project arcs\\lib\\site-packages\\gensim\\utils.py:1268: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected %s; aliasing chunkize to chunkize_serial\" % entity)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Doc2Vec(dbow+w,d300,n5,w8,mc50,s0.001,t12)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "models[0].build_vocab(documents)\n",
    "print(str(models[0]))\n",
    "# models[1].reset_from(models[0])\n",
    "# print(str(models[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we’re ready to train Doc2Vec of the English Wikipedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for model in models:\n",
    "    %%time model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\project arcs\\lib\\site-packages\\gensim\\utils.py:1268: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected %s; aliasing chunkize to chunkize_serial\" % entity)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Wall time: 2d 6min 24s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%%time \n",
    "models[0].train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save trained model "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fname = \"F:/Models/doc2vec_wiki_d300_n5_w8_mc50_t12_e10_dbow.model\"\n",
    "models[0].save(fname)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarity interface"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "fname = \"F:/Models/doc2vec_wiki_d300_n5_w8_mc50_t12_e10_dbow.model\"\n",
    "models[0].save(fname)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarity interface"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, let's test both models! DBOW model show similar results with the original paper. First, calculating cosine similarity of \"Machine learning\" using Paragraph Vector. Word Vector and Document Vector are separately stored. We have to add .docvecs after model name to extract Document Vector from Doc2Vec Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,hs,w8,mc19,t8)\n",
      "[('Theoretical computer science', 0.7256590127944946),\n",
      " ('Artificial neural network', 0.7162272930145264),\n",
      " ('Pattern recognition', 0.6948175430297852),\n",
      " ('Data mining', 0.6938608884811401),\n",
      " ('Bayesian network', 0.6938260197639465),\n",
      " ('Support vector machine', 0.6706081628799438),\n",
      " ('Glossary of artificial intelligence', 0.670173704624176),\n",
      " ('Computational learning theory', 0.6648679971694946),\n",
      " ('Outline of computer science', 0.6638073921203613),\n",
      " ('List of important publications in computer science', 0.663051187992096),\n",
      " ('Mathematical optimization', 0.655048131942749),\n",
      " ('Theory of computation', 0.6508707404136658),\n",
      " ('Word-sense disambiguation', 0.6505812406539917),\n",
      " ('Reinforcement learning', 0.6480429172515869),\n",
      " (\"Solomonoff's theory of inductive inference\", 0.6459559202194214),\n",
      " ('Computational intelligence', 0.6458009481430054),\n",
      " ('Information visualization', 0.6437181234359741),\n",
      " ('Algorithmic composition', 0.643247127532959),\n",
      " ('Ray Solomonoff', 0.6425477862358093),\n",
      " ('Kriging', 0.6425424814224243)]\n",
      "Doc2Vec(dm/m,d200,hs,w8,mc19,t8)\n",
      "[('Artificial neural network', 0.640324592590332),\n",
      " ('Pattern recognition', 0.6244156360626221),\n",
      " ('Data stream mining', 0.6140210032463074),\n",
      " ('Theoretical computer science', 0.5964258909225464),\n",
      " ('Outline of computer science', 0.5862746834754944),\n",
      " ('Supervised learning', 0.5847170352935791),\n",
      " ('Data mining', 0.5817658305168152),\n",
      " ('Decision tree learning', 0.5785809755325317),\n",
      " ('Bayesian network', 0.5768401622772217),\n",
      " ('Computational intelligence', 0.5717238187789917),\n",
      " ('Theory of computation', 0.5703311562538147),\n",
      " ('Bayesian programming', 0.5693561434745789),\n",
      " ('Reinforcement learning', 0.564978837966919),\n",
      " ('Helmholtz machine', 0.564972460269928),\n",
      " ('Inductive logic programming', 0.5631471276283264),\n",
      " ('Algorithmic learning theory', 0.563083291053772),\n",
      " ('Semi-supervised learning', 0.5628935694694519),\n",
      " ('Early stopping', 0.5597405433654785),\n",
      " ('Decision tree', 0.5596889853477478),\n",
      " ('Artificial intelligence', 0.5569720268249512)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(str(model))\n",
    "    pprint(model.docvecs.most_similar(positive=[\"Machine learning\"], topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBOW model interpret the word 'Machine Learning' as a part of Computer Science field, and DM model as Data Science related field.\n",
    "\n",
    "Second, calculating cosine simillarity of \"Lady Gaga\" using Paragraph Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,hs,w8,mc19,t8)\n",
      "[('Katy Perry', 0.7374469637870789),\n",
      " ('Adam Lambert', 0.6972734928131104),\n",
      " ('Miley Cyrus', 0.6212848424911499),\n",
      " ('List of awards and nominations received by Lady Gaga', 0.6138384938240051),\n",
      " ('Nicole Scherzinger', 0.6092700958251953),\n",
      " ('Christina Aguilera', 0.6062655448913574),\n",
      " ('Nicki Minaj', 0.6019431948661804),\n",
      " ('Taylor Swift', 0.5973174571990967),\n",
      " ('The Pussycat Dolls', 0.5888757705688477),\n",
      " ('Beyoncé', 0.5844652652740479)]\n",
      "Doc2Vec(dm/m,d200,hs,w8,mc19,t8)\n",
      "[('ArtRave: The Artpop Ball', 0.5719832181930542),\n",
      " ('Artpop', 0.5651129484176636),\n",
      " ('Katy Perry', 0.5571318864822388),\n",
      " ('The Fame', 0.5388195514678955),\n",
      " ('The Fame Monster', 0.5380634069442749),\n",
      " ('G.U.Y.', 0.5365751385688782),\n",
      " ('Beautiful, Dirty, Rich', 0.5329179763793945),\n",
      " ('Applause (Lady Gaga song)', 0.5328119993209839),\n",
      " ('The Monster Ball Tour', 0.5299569368362427),\n",
      " ('Lindsey Stirling', 0.5281971096992493)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(str(model))\n",
    "    pprint(model.docvecs.most_similar(positive=[\"Lady Gaga\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "DBOW model reveal the similar singer in the U.S., and DM model understand that many of Lady Gaga's songs are similar with the word \"Lady Gaga\".\n",
    "\n",
    "Third, calculating cosine simillarity of \"Lady Gaga\" - \"American\" + \"Japanese\" using Document vector and Word Vectors. \"American\" and \"Japanese\" are Word Vectors, not Paragraph Vectors. Word Vectors are already converted to lowercases by WikiCorpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,hs,w8,mc19,t8)\n",
      "[('Game (Perfume album)', 0.5571034550666809),\n",
      " ('Katy Perry', 0.5537782311439514),\n",
      " ('Taboo (Kumi Koda song)', 0.5304880142211914),\n",
      " ('Kylie Minogue', 0.5234110355377197),\n",
      " ('Ayumi Hamasaki', 0.5110630989074707),\n",
      " (\"Girls' Generation\", 0.4996713399887085),\n",
      " ('Britney Spears', 0.49094343185424805),\n",
      " ('Koda Kumi', 0.48719698190689087),\n",
      " ('Perfume (Japanese band)', 0.48536181449890137),\n",
      " ('Kara (South Korean band)', 0.48507893085479736)]\n",
      "Doc2Vec(dm/m,d200,hs,w8,mc19,t8)\n",
      "[('Artpop', 0.47699037194252014),\n",
      " ('Jessie J', 0.4439432621002197),\n",
      " ('Haus of Gaga', 0.43463900685310364),\n",
      " ('The Fame', 0.4278091788291931),\n",
      " ('List of awards and nominations received by Lady Gaga', 0.4268512427806854),\n",
      " ('Applause (Lady Gaga song)', 0.41563737392425537),\n",
      " ('New Cutie Honey', 0.4152414798736572),\n",
      " ('M.I.A. (rapper)', 0.4091864228248596),\n",
      " ('Mama Do (Uh Oh, Uh Oh)', 0.4044945538043976),\n",
      " ('The Fame Monster', 0.40421998500823975)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(str(model))\n",
    "    vec = [model.docvecs[\"Lady Gaga\"] - model[\"american\"] + model[\"japanese\"]]\n",
    "    pprint([m for m in model.docvecs.most_similar(vec, topn=11) if m[0] != \"Lady Gaga\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, DBOW model demonstrate similar artists to Lady Gaga in Japan such as 'Perfume', who is the most famous idol in Japan. On the other hand, DM model results don't include Japanese artists in top 10 similar documents. It's almost the same with no vector calculated results.\n",
    "\n",
    "These results demonstrate that the DBOW employed in the original paper is outstanding for calculating similarity between Document Vector and Word Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[ 0.21061182 -0.16099742 -0.05722008  0.481086   -0.1445539  -0.04800414\n",
      " -0.04645299  0.06052135 -0.06470083  0.23517233 -0.06187142 -0.2051702\n",
      " -0.25905907 -0.31462678 -0.01948082  0.02639042 -0.18474741  0.21840176\n",
      " -0.07172099  0.24462838 -0.04338861 -0.26034835 -0.2508871   0.7233908\n",
      " -0.242208    0.33711162  0.37152743 -0.43784922 -0.06018984  0.09739308\n",
      " -0.00657956  0.35021454  0.20945679  0.05264752  0.14791027  0.12886657\n",
      " -0.11360193  0.11213093 -0.04108432 -0.3440291   0.09316082 -0.4917564\n",
      " -0.38420203  0.38431576  0.2545955  -0.08729217 -0.41765788  0.16834158\n",
      " -0.20573752 -0.2221307  -0.13681568  0.2254329   0.465276   -0.00140286\n",
      " -0.30348358  0.10881878  0.14127888  0.04611235 -0.15430973 -0.19094457\n",
      "  0.2689548  -0.16394477  0.2481665  -0.28256294 -0.04128566  0.17052698\n",
      "  0.20022848  0.2404979  -0.22429691 -0.06907493  0.15592222 -0.31730628\n",
      "  0.02741473 -0.26515883 -0.22088528  0.09564014  0.2972641   0.140198\n",
      "  0.17578658  0.13320044 -0.03595469  0.53154516 -0.65689325 -0.37945214\n",
      " -0.4637075   0.17695154 -0.24740222  0.3993498  -0.28035724 -0.22549246\n",
      " -0.19520015  0.15845057  0.04058439  0.15814643  0.42180693 -0.27693394\n",
      " -0.342954    0.13487926  0.03432916  0.34663185  0.04810831  0.01200578\n",
      "  0.11988313  0.7539272   0.51227933 -0.19312757  0.32626912 -0.13511997\n",
      "  0.02356322 -0.10125876 -0.00539503 -0.5123771  -0.11988942 -0.13714693\n",
      " -0.1386931  -0.0783608   0.05521704 -0.17615075  0.09447098 -0.24743445\n",
      "  0.01230258 -0.5705255   0.12435693 -0.33576965 -0.36381066 -0.1568926\n",
      " -0.01382011 -0.10600963  0.5541947  -0.26233324 -0.06906062 -0.16079314\n",
      " -0.38484824  0.11607866  0.37607995  0.02405544  0.134529    0.27645227\n",
      "  0.18326339  0.18271463  0.4208948  -0.2923201   0.0807279   0.25470516\n",
      "  0.20804435  0.08106102 -0.18774264  0.1499281   0.24714646  0.27246246\n",
      "  0.02328494  0.17787881 -0.06994545  0.05474208  0.01940906 -0.10671407\n",
      " -0.2554522   0.1882909  -0.1795709   0.20446722 -0.27729297  0.10198469\n",
      "  0.04760354 -0.42765462 -0.26414138  0.0190211  -0.27858743 -0.7378304\n",
      "  0.10610525  0.6685537   0.11868842 -0.20393062  0.3565453  -0.1252641\n",
      "  0.01630444 -0.03687834 -0.22564057 -0.21688576  0.21285675  0.53417134\n",
      " -0.07343839  0.32164854  0.12441431 -0.05531272  0.0488361  -0.48006418\n",
      "  0.08953179  0.24338192 -0.2928486  -0.04930811 -0.5738481   0.05554447\n",
      "  0.02022559  0.07626417  0.1502923   0.24265078 -0.03130227 -0.2130532\n",
      "  0.31516367  0.01965449  0.3530494   0.46684715 -0.29242104 -0.14590915\n",
      "  0.27773222 -0.01000386 -0.16940162  0.0426551   0.1657033   0.3542437\n",
      " -0.24055882  0.1590615   0.11435834 -0.06751151 -0.00414594 -0.36964732\n",
      "  0.19522367  0.23684238  0.2388869   0.41413862  0.49400878 -0.09707638\n",
      "  0.09116849  0.0621034  -0.07639633 -0.14709102 -0.12843321 -0.435531\n",
      "  0.3273767   0.19302318 -0.07460678  0.25029507 -0.5284614  -0.28210238\n",
      "  0.09107192  0.07227308  0.27666324 -0.21285196 -0.21246178 -0.44374925\n",
      "  0.47797212  0.5467655   0.18835881 -0.24007991  0.17547192 -0.1001187\n",
      "  0.10972873  0.3530887   0.19911338 -0.18856019  0.1718035  -0.6338441\n",
      " -0.00194543 -0.3982743  -0.07928567 -0.33082047  0.2622803  -0.01626496\n",
      "  0.11701803  0.11382837  0.06350155  0.37175208  0.04003908 -0.11448162\n",
      "  0.3371895   0.76896536  0.01834371 -0.19742939  0.07956657  0.17510524\n",
      " -0.39027107 -0.3785085   0.43321142  0.02558968  0.19633488  0.10228845\n",
      "  0.09053521 -0.34694898  0.14172833  0.09055211 -0.47578773 -0.27941465\n",
      " -0.02317569  0.38024166  0.18189277  0.2902913  -0.12949698 -0.05700676\n",
      " -0.17153177 -0.00964904 -0.10009977 -0.22198449  0.2256355  -0.31439486\n",
      "  0.4524467  -0.26662084  0.28363553 -0.47582138 -0.0766274  -0.08045685]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(model.docvecs[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}